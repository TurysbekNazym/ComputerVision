{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern recognition: Lab 7\n",
    "### Tasks:\n",
    "* Plot the error\n",
    "* Model XOR with the help of sigmoid\n",
    "* Add moments rule to learning equation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "k = 1\n",
    "def sigmoid(x):\n",
    "    return 1.0/(1.0 + np.exp(-k*x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    return x*(1.0-x)\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_prime(x):\n",
    "    return 1.0 - x**2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs: 0\n",
      "epochs: 10000\n",
      "epochs: 20000\n",
      "epochs: 30000\n",
      "epochs: 40000\n",
      "epochs: 50000\n",
      "epochs: 60000\n",
      "epochs: 70000\n",
      "epochs: 80000\n",
      "epochs: 90000\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEKCAYAAAAB0GKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x14abd088c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0] [ 0.01774741]\n",
      "[0 1] [ 0.98895161]\n",
      "[1 0] [ 0.98081095]\n",
      "[1 1] [ 0.01656589]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as pltimg\n",
    "\n",
    "class NeuralNetwork:\n",
    "\n",
    "    def __init__(self, layers):\n",
    "        self.activation = sigmoid\n",
    "        self.activation_prime = sigmoid_prime\n",
    "\n",
    "        # Set weights\n",
    "        self.weights = []\n",
    "        # layers = [2,2,1]\n",
    "        # range of weight values (-1,1)\n",
    "        # input and hidden layers - random((2+1, 2+1)) : 3 x 3\n",
    "        \n",
    "        for i in range(1, len(layers) - 1):\n",
    "            r = 2*np.random.random((layers[i-1] + 1, layers[i] + 1)) -1\n",
    "            self.weights.append(r)\n",
    "        # output layer - random((2+1, 1)) : 3 x 1\n",
    "        r = 2*np.random.random( (layers[i] + 1, layers[i+1])) - 1\n",
    "        self.weights.append(r)\n",
    "  \n",
    "    def fit(self, X, y, learning_rate=0.2, epochs=100000):\n",
    "        # Add column of ones to X\n",
    "        # This is to add the bias unit to the input layer\n",
    "        ones = np.atleast_2d(np.ones(X.shape[0]))\n",
    "        X = np.concatenate((ones.T, X), axis=1)\n",
    "        myList=[]\n",
    "        avList=[]\n",
    "       \n",
    "         \n",
    "        for k in range(epochs):\n",
    "            i = np.random.randint(X.shape[0])\n",
    "            a = [X[i]]\n",
    "            a1 = [X[i]]\n",
    "\n",
    "            for l in range(len(self.weights)):\n",
    "                    dot_value = np.dot(a[l], self.weights[l])\n",
    "                    activation = self.activation(dot_value)\n",
    "                    a.append(activation)\n",
    "            # output layer\n",
    "      \n",
    "           \n",
    "            error = y[i] - a[-1]\n",
    "            myList.append(error**2)# mean squard error MSE  (error)\n",
    "            deltas = [error * self.activation_prime(a[-1])]\n",
    "            \n",
    "                \n",
    "          \n",
    "\n",
    "\n",
    "            # we need to begin at the second to last layer \n",
    "            # (a layer before the output layer)\n",
    "            for l in range(len(a) - 2, 0, -1):\n",
    "                deltas.append(deltas[-1].dot(self.weights[l].T)*self.activation_prime(a[l]))#error*f`(x)*w\n",
    "\n",
    "            # reverse\n",
    "            # [level3(output)->level2(hidden)]  => [level2(hidden)->level3(output)]\n",
    "            deltas.reverse()\n",
    "\n",
    "            # backpropagation\n",
    "            # 1. Multiply its output delta and input activation \n",
    "            #    to get the gradient of the weight.\n",
    "            # 2. Subtract a ratio (percentage) of the gradient from the weight.\n",
    "           \n",
    "        \n",
    "            for i in range(len(self.weights)):\n",
    "                 \n",
    "                    layer = np.atleast_2d(a[i])\n",
    "                    delta = np.atleast_2d(deltas[i])\n",
    "                # gradient + = layer.T.dot(delta)\n",
    "                    self.weights[i] = self.weights[i] + learning_rate*layer.T.dot(delta)\n",
    "                \n",
    "  \n",
    "            if k % 10000 == 0: \n",
    "                print('epochs:', k)\n",
    "                t = np.average(myList)\n",
    "                avList.append(t)\n",
    "           \n",
    "           #plt.show()\n",
    "        #print(myList)\n",
    "        \n",
    "        plt.plot(myList[1])\n",
    "        plt.plot(avList)\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('error')\n",
    "        plt.show()\n",
    "    \n",
    "    def predict(self, x): \n",
    "    \n",
    "        a = np.concatenate((np.ones(1).T, np.array(x)))      \n",
    "\n",
    "        for l in range(0, len(self.weights)):\n",
    "            a = self.activation(np.dot(a, self.weights[l]))\n",
    "        return a\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    nn = NeuralNetwork([2,2,1])\n",
    "    X = np.array([[0, 0],\n",
    "                  [0, 1],\n",
    "                  [1, 0],\n",
    "                  [1, 1]])\n",
    "    y = np.array([0, 1, 1, 0])\n",
    "#     X = np.array([[-1, -1],\n",
    "#                   [-1, 1],\n",
    "#                   [1, -1],\n",
    "\n",
    "#                   [1, 1]])\n",
    "#     y = np.array([0, 1, 1, 0])\n",
    "\n",
    "    nn.fit(X, y)\n",
    "    for e in X:\n",
    "        print(e,nn.predict(e))\n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apply_nesterov_momentum(updates, params=None, momentum=0.9):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-5-987201ca294f>, line 35)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-5-987201ca294f>\"\u001b[1;36m, line \u001b[1;32m35\u001b[0m\n\u001b[1;33m    print 'Epoch {0}: {1} / {2}',format(\u001b[0m\n\u001b[1;37m                               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Third-party libraries\n",
    "import numpy as np\n",
    "\n",
    "class Network(object):\n",
    "\n",
    "    def __init__(self, sizes):\n",
    "      \n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)\n",
    "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "\n",
    "    def feedforward(self, a):\n",
    "       \n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a)+b)\n",
    "        return a\n",
    "\n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
    "            test_data=None):\n",
    "      \n",
    "        if test_data: n_test = len(test_data)\n",
    "        n = len(training_data)\n",
    "        for j in xrange(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [\n",
    "                training_data[k:k+mini_batch_size]\n",
    "                for k in xrange(0, n, mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "            if test_data:\n",
    "                print 'Epoch {0}: {1} / {2}',format(\n",
    "                    j, self.evaluate(test_data), n_test)\n",
    "            else:\n",
    "                print 'Epoch {0} complete',format(j)\n",
    "\n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "       \n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        self.weights = [w-(eta/len(mini_batch))*nw\n",
    "                        for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(eta/len(mini_batch))*nb\n",
    "                       for b, nb in zip(self.biases, nabla_b)]\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "      \n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x] # list to store all the activations, layer by layer\n",
    "        zs = [] # list to store all the z vectors, layer by layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        # backward pass\n",
    "        delta = self.cost_derivative(activations[-1], y) * \\\n",
    "            sigmoid_prime(zs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        # Note that the variable l in the loop below is used a little\n",
    "        # differently to the notation in Chapter 2 of the book.  Here,\n",
    "        # l = 1 means the last layer of neurons, l = 2 is the\n",
    "        # second-last layer, and so on.  It's a renumbering of the\n",
    "        # scheme in the book, used here to take advantage of the fact\n",
    "        # that Python can use negative indices in lists.\n",
    "        for l in xrange(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "\n",
    "    def evaluate(self, test_data):\n",
    "\n",
    "        test_results = [(np.argmax(self.feedforward(x)), y)\n",
    "                        for (x, y) in test_data]\n",
    "        return sum(int(x == y) for (x, y) in test_results)\n",
    "\n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        \n",
    "        return (output_activations-y)\n",
    "\n",
    "#### Miscellaneous functions\n",
    "def sigmoid(z):\n",
    "   \n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "   \n",
    "    return sigmoid(z)*(1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print' (<ipython-input-8-b452286f0f11>, line 142)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-8-b452286f0f11>\"\u001b[1;36m, line \u001b[1;32m142\u001b[0m\n\u001b[1;33m    print \"Epoch %s training complete\" % j\u001b[0m\n\u001b[1;37m                                     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m Missing parentheses in call to 'print'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import sys\n",
    "\n",
    "# Third-party libraries\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#### Define the quadratic and cross-entropy cost functions\n",
    "\n",
    "class QuadraticCost(object):\n",
    "\n",
    "    @staticmethod\n",
    "    def fn(a, y):\n",
    "        \"\"\"Return the cost associated with an output ``a`` and desired output\n",
    "        ``y``.\n",
    "        \"\"\"\n",
    "        return 0.5*np.linalg.norm(a-y)**2\n",
    "\n",
    "    @staticmethod\n",
    "    def delta(z, a, y):\n",
    "        \"\"\"Return the error delta from the output layer.\"\"\"\n",
    "        return (a-y) * sigmoid_prime(z)\n",
    "\n",
    "\n",
    "class CrossEntropyCost(object):\n",
    "\n",
    "    @staticmethod\n",
    "    def fn(a, y):\n",
    "        \"\"\"Return the cost associated with an output ``a`` and desired output\n",
    "        ``y``.  Note that np.nan_to_num is used to ensure numerical\n",
    "        stability.  In particular, if both ``a`` and ``y`` have a 1.0\n",
    "        in the same slot, then the expression (1-y)*np.log(1-a)\n",
    "        returns nan.  The np.nan_to_num ensures that that is converted\n",
    "        to the correct value (0.0).\n",
    "        \"\"\"\n",
    "        return np.sum(np.nan_to_num(-y*np.log(a)-(1-y)*np.log(1-a)))\n",
    "\n",
    "    @staticmethod\n",
    "    def delta(z, a, y):\n",
    "        \"\"\"Return the error delta from the output layer.  Note that the\n",
    "        parameter ``z`` is not used by the method.  It is included in\n",
    "        the method's parameters in order to make the interface\n",
    "        consistent with the delta method for other cost classes.\n",
    "        \"\"\"\n",
    "        return (a-y)\n",
    "\n",
    "\n",
    "#### Main Network class\n",
    "class Network(object):\n",
    "\n",
    "    def __init__(self, sizes, cost=CrossEntropyCost):\n",
    "        \"\"\"The list ``sizes`` contains the number of neurons in the respective\n",
    "        layers of the network.  For example, if the list was [2, 3, 1]\n",
    "        then it would be a three-layer network, with the first layer\n",
    "        containing 2 neurons, the second layer 3 neurons, and the\n",
    "        third layer 1 neuron.  The biases and weights for the network\n",
    "        are initialized randomly, using\n",
    "        ``self.default_weight_initializer`` (see docstring for that\n",
    "        method).\n",
    "        \"\"\"\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.default_weight_initializer()\n",
    "        self.cost=cost\n",
    "\n",
    "    def default_weight_initializer(self):\n",
    "        \"\"\"Initialize each weight using a Gaussian distribution with mean 0\n",
    "        and standard deviation 1 over the square root of the number of\n",
    "        weights connecting to the same neuron.  Initialize the biases\n",
    "        using a Gaussian distribution with mean 0 and standard\n",
    "        deviation 1.\n",
    "        Note that the first layer is assumed to be an input layer, and\n",
    "        by convention we won't set any biases for those neurons, since\n",
    "        biases are only ever used in computing the outputs from later\n",
    "        layers.\n",
    "        \"\"\"\n",
    "        self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)/np.sqrt(x)\n",
    "                        for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "\n",
    "    def large_weight_initializer(self):\n",
    "        \"\"\"Initialize the weights using a Gaussian distribution with mean 0\n",
    "        and standard deviation 1.  Initialize the biases using a\n",
    "        Gaussian distribution with mean 0 and standard deviation 1.\n",
    "        Note that the first layer is assumed to be an input layer, and\n",
    "        by convention we won't set any biases for those neurons, since\n",
    "        biases are only ever used in computing the outputs from later\n",
    "        layers.\n",
    "        This weight and bias initializer uses the same approach as in\n",
    "        Chapter 1, and is included for purposes of comparison.  It\n",
    "        will usually be better to use the default weight initializer\n",
    "        instead.\n",
    "        \"\"\"\n",
    "        self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)\n",
    "                        for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "\n",
    "    def feedforward(self, a):\n",
    "        \"\"\"Return the output of the network if ``a`` is input.\"\"\"\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a)+b)\n",
    "        return a\n",
    "\n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
    "            lmbda = 0.0,\n",
    "            evaluation_data=None,\n",
    "            monitor_evaluation_cost=False,\n",
    "            monitor_evaluation_accuracy=False,\n",
    "            monitor_training_cost=False,\n",
    "            monitor_training_accuracy=False):\n",
    "        \"\"\"Train the neural network using mini-batch stochastic gradient\n",
    "        descent.  The ``training_data`` is a list of tuples ``(x, y)``\n",
    "        representing the training inputs and the desired outputs.  The\n",
    "        other non-optional parameters are self-explanatory, as is the\n",
    "        regularization parameter ``lmbda``.  The method also accepts\n",
    "        ``evaluation_data``, usually either the validation or test\n",
    "        data.  We can monitor the cost and accuracy on either the\n",
    "        evaluation data or the training data, by setting the\n",
    "        appropriate flags.  The method returns a tuple containing four\n",
    "        lists: the (per-epoch) costs on the evaluation data, the\n",
    "        accuracies on the evaluation data, the costs on the training\n",
    "        data, and the accuracies on the training data.  All values are\n",
    "        evaluated at the end of each training epoch.  So, for example,\n",
    "        if we train for 30 epochs, then the first element of the tuple\n",
    "        will be a 30-element list containing the cost on the\n",
    "        evaluation data at the end of each epoch. Note that the lists\n",
    "        are empty if the corresponding flag is not set.\n",
    "        \"\"\"\n",
    "        if evaluation_data: n_data = len(evaluation_data)\n",
    "        n = len(training_data)\n",
    "        evaluation_cost, evaluation_accuracy = [], []\n",
    "        training_cost, training_accuracy = [], []\n",
    "        for j in xrange(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [\n",
    "                training_data[k:k+mini_batch_size]\n",
    "                for k in xrange(0, n, mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(\n",
    "                    mini_batch, eta, lmbda, len(training_data))\n",
    "            print \"Epoch %s training complete\" % j\n",
    "            if monitor_training_cost:\n",
    "                cost = self.total_cost(training_data, lmbda)\n",
    "                training_cost.append(cost)\n",
    "                print \"Cost on training data: {}\".format(cost)\n",
    "            if monitor_training_accuracy:\n",
    "                accuracy = self.accuracy(training_data, convert=True)\n",
    "                training_accuracy.append(accuracy)\n",
    "                print \"Accuracy on training data: {} / {}\".format(\n",
    "                    accuracy, n)\n",
    "            if monitor_evaluation_cost:\n",
    "                cost = self.total_cost(evaluation_data, lmbda, convert=True)\n",
    "                evaluation_cost.append(cost)\n",
    "                print \"Cost on evaluation data: {}\".format(cost)\n",
    "            if monitor_evaluation_accuracy:\n",
    "                accuracy = self.accuracy(evaluation_data)\n",
    "                evaluation_accuracy.append(accuracy)\n",
    "                print \"Accuracy on evaluation data: {} / {}\".format(\n",
    "                    self.accuracy(evaluation_data), n_data)\n",
    "            print\n",
    "        return evaluation_cost, evaluation_accuracy, \\\n",
    "            training_cost, training_accuracy\n",
    "\n",
    "    def update_mini_batch(self, mini_batch, eta, lmbda, n):\n",
    "        \"\"\"Update the network's weights and biases by applying gradient\n",
    "        descent using backpropagation to a single mini batch.  The\n",
    "        ``mini_batch`` is a list of tuples ``(x, y)``, ``eta`` is the\n",
    "        learning rate, ``lmbda`` is the regularization parameter, and\n",
    "        ``n`` is the total size of the training data set.\n",
    "        \"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        self.weights = [(1-eta*(lmbda/n))*w-(eta/len(mini_batch))*nw\n",
    "                        for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(eta/len(mini_batch))*nb\n",
    "                       for b, nb in zip(self.biases, nabla_b)]\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "        \"\"\"Return a tuple ``(nabla_b, nabla_w)`` representing the\n",
    "        gradient for the cost function C_x.  ``nabla_b`` and\n",
    "        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar\n",
    "        to ``self.biases`` and ``self.weights``.\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x] # list to store all the activations, layer by layer\n",
    "        zs = [] # list to store all the z vectors, layer by layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        # backward pass\n",
    "        delta = (self.cost).delta(zs[-1], activations[-1], y)\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        # Note that the variable l in the loop below is used a little\n",
    "        # differently to the notation in Chapter 2 of the book.  Here,\n",
    "        # l = 1 means the last layer of neurons, l = 2 is the\n",
    "        # second-last layer, and so on.  It's a renumbering of the\n",
    "        # scheme in the book, used here to take advantage of the fact\n",
    "        # that Python can use negative indices in lists.\n",
    "        for l in xrange(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "\n",
    "    def accuracy(self, data, convert=False):\n",
    "        \"\"\"Return the number of inputs in ``data`` for which the neural\n",
    "        network outputs the correct result. The neural network's\n",
    "        output is assumed to be the index of whichever neuron in the\n",
    "        final layer has the highest activation.\n",
    "        The flag ``convert`` should be set to False if the data set is\n",
    "        validation or test data (the usual case), and to True if the\n",
    "        data set is the training data. The need for this flag arises\n",
    "        due to differences in the way the results ``y`` are\n",
    "        represented in the different data sets.  In particular, it\n",
    "        flags whether we need to convert between the different\n",
    "        representations.  It may seem strange to use different\n",
    "        representations for the different data sets.  Why not use the\n",
    "        same representation for all three data sets?  It's done for\n",
    "        efficiency reasons -- the program usually evaluates the cost\n",
    "        on the training data and the accuracy on other data sets.\n",
    "        These are different types of computations, and using different\n",
    "        representations speeds things up.  More details on the\n",
    "        representations can be found in\n",
    "        mnist_loader.load_data_wrapper.\n",
    "        \"\"\"\n",
    "        if convert:\n",
    "            results = [(np.argmax(self.feedforward(x)), np.argmax(y))\n",
    "                       for (x, y) in data]\n",
    "        else:\n",
    "            results = [(np.argmax(self.feedforward(x)), y)\n",
    "                        for (x, y) in data]\n",
    "        return sum(int(x == y) for (x, y) in results)\n",
    "\n",
    "    def total_cost(self, data, lmbda, convert=False):\n",
    "        \"\"\"Return the total cost for the data set ``data``.  The flag\n",
    "        ``convert`` should be set to False if the data set is the\n",
    "        training data (the usual case), and to True if the data set is\n",
    "        the validation or test data.  See comments on the similar (but\n",
    "        reversed) convention for the ``accuracy`` method, above.\n",
    "        \"\"\"\n",
    "        cost = 0.0\n",
    "        for x, y in data:\n",
    "            a = self.feedforward(x)\n",
    "            if convert: y = vectorized_result(y)\n",
    "            cost += self.cost.fn(a, y)/len(data)\n",
    "        cost += 0.5*(lmbda/len(data))*sum(\n",
    "            np.linalg.norm(w)**2 for w in self.weights)\n",
    "        return cost\n",
    "\n",
    "    def save(self, filename):\n",
    "        \"\"\"Save the neural network to the file ``filename``.\"\"\"\n",
    "        data = {\"sizes\": self.sizes,\n",
    "                \"weights\": [w.tolist() for w in self.weights],\n",
    "                \"biases\": [b.tolist() for b in self.biases],\n",
    "                \"cost\": str(self.cost.__name__)}\n",
    "        f = open(filename, \"w\")\n",
    "        json.dump(data, f)\n",
    "        f.close()\n",
    "\n",
    "#### Loading a Network\n",
    "def load(filename):\n",
    "    \"\"\"Load a neural network from the file ``filename``.  Returns an\n",
    "    instance of Network.\n",
    "    \"\"\"\n",
    "    f = open(filename, \"r\")\n",
    "    data = json.load(f)\n",
    "    f.close()\n",
    "    cost = getattr(sys.modules[__name__], data[\"cost\"])\n",
    "    net = Network(data[\"sizes\"], cost=cost)\n",
    "    net.weights = [np.array(w) for w in data[\"weights\"]]\n",
    "    net.biases = [np.array(b) for b in data[\"biases\"]]\n",
    "    return net\n",
    "\n",
    "#### Miscellaneous functions\n",
    "def vectorized_result(j):\n",
    "    \"\"\"Return a 10-dimensional unit vector with a 1.0 in the j'th position\n",
    "    and zeroes elsewhere.  This is used to convert a digit (0...9)\n",
    "    into a corresponding desired output from the neural network.\n",
    "    \"\"\"\n",
    "    e = np.zeros((10, 1))\n",
    "    e[j] = 1.0\n",
    "    return e\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"The sigmoid function.\"\"\"\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    return sigmoid(z)*(1-sigmoid(z))import json\n",
    "import random\n",
    "import sys\n",
    "\n",
    "# Third-party libraries\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#### Define the quadratic and cross-entropy cost functions\n",
    "\n",
    "class QuadraticCost(object):\n",
    "\n",
    "    @staticmethod\n",
    "    def fn(a, y):\n",
    "        \"\"\"Return the cost associated with an output ``a`` and desired output\n",
    "        ``y``.\n",
    "        \"\"\"\n",
    "        return 0.5*np.linalg.norm(a-y)**2\n",
    "\n",
    "    @staticmethod\n",
    "    def delta(z, a, y):\n",
    "        \"\"\"Return the error delta from the output layer.\"\"\"\n",
    "        return (a-y) * sigmoid_prime(z)\n",
    "\n",
    "\n",
    "class CrossEntropyCost(object):\n",
    "\n",
    "    @staticmethod\n",
    "    def fn(a, y):\n",
    "        \"\"\"Return the cost associated with an output ``a`` and desired output\n",
    "        ``y``.  Note that np.nan_to_num is used to ensure numerical\n",
    "        stability.  In particular, if both ``a`` and ``y`` have a 1.0\n",
    "        in the same slot, then the expression (1-y)*np.log(1-a)\n",
    "        returns nan.  The np.nan_to_num ensures that that is converted\n",
    "        to the correct value (0.0).\n",
    "        \"\"\"\n",
    "        return np.sum(np.nan_to_num(-y*np.log(a)-(1-y)*np.log(1-a)))\n",
    "\n",
    "    @staticmethod\n",
    "    def delta(z, a, y):\n",
    "        \"\"\"Return the error delta from the output layer.  Note that the\n",
    "        parameter ``z`` is not used by the method.  It is included in\n",
    "        the method's parameters in order to make the interface\n",
    "        consistent with the delta method for other cost classes.\n",
    "        \"\"\"\n",
    "        return (a-y)\n",
    "\n",
    "\n",
    "#### Main Network class\n",
    "class Network(object):\n",
    "\n",
    "    def __init__(self, sizes, cost=CrossEntropyCost):\n",
    "        \"\"\"The list ``sizes`` contains the number of neurons in the respective\n",
    "        layers of the network.  For example, if the list was [2, 3, 1]\n",
    "        then it would be a three-layer network, with the first layer\n",
    "        containing 2 neurons, the second layer 3 neurons, and the\n",
    "        third layer 1 neuron.  The biases and weights for the network\n",
    "        are initialized randomly, using\n",
    "        ``self.default_weight_initializer`` (see docstring for that\n",
    "        method).\n",
    "        \"\"\"\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.default_weight_initializer()\n",
    "        self.cost=cost\n",
    "\n",
    "    def default_weight_initializer(self):\n",
    "        \"\"\"Initialize each weight using a Gaussian distribution with mean 0\n",
    "        and standard deviation 1 over the square root of the number of\n",
    "        weights connecting to the same neuron.  Initialize the biases\n",
    "        using a Gaussian distribution with mean 0 and standard\n",
    "        deviation 1.\n",
    "        Note that the first layer is assumed to be an input layer, and\n",
    "        by convention we won't set any biases for those neurons, since\n",
    "        biases are only ever used in computing the outputs from later\n",
    "        layers.\n",
    "        \"\"\"\n",
    "        self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)/np.sqrt(x)\n",
    "                        for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "\n",
    "    def large_weight_initializer(self):\n",
    "        \"\"\"Initialize the weights using a Gaussian distribution with mean 0\n",
    "        and standard deviation 1.  Initialize the biases using a\n",
    "        Gaussian distribution with mean 0 and standard deviation 1.\n",
    "        Note that the first layer is assumed to be an input layer, and\n",
    "        by convention we won't set any biases for those neurons, since\n",
    "        biases are only ever used in computing the outputs from later\n",
    "        layers.\n",
    "        This weight and bias initializer uses the same approach as in\n",
    "        Chapter 1, and is included for purposes of comparison.  It\n",
    "        will usually be better to use the default weight initializer\n",
    "        instead.\n",
    "        \"\"\"\n",
    "        self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)\n",
    "                        for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "\n",
    "    def feedforward(self, a):\n",
    "        \"\"\"Return the output of the network if ``a`` is input.\"\"\"\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a)+b)\n",
    "        return a\n",
    "\n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
    "            lmbda = 0.0,\n",
    "            evaluation_data=None,\n",
    "            monitor_evaluation_cost=False,\n",
    "            monitor_evaluation_accuracy=False,\n",
    "            monitor_training_cost=False,\n",
    "            monitor_training_accuracy=False):\n",
    "        \"\"\"Train the neural network using mini-batch stochastic gradient\n",
    "        descent.  The ``training_data`` is a list of tuples ``(x, y)``\n",
    "        representing the training inputs and the desired outputs.  The\n",
    "        other non-optional parameters are self-explanatory, as is the\n",
    "        regularization parameter ``lmbda``.  The method also accepts\n",
    "        ``evaluation_data``, usually either the validation or test\n",
    "        data.  We can monitor the cost and accuracy on either the\n",
    "        evaluation data or the training data, by setting the\n",
    "        appropriate flags.  The method returns a tuple containing four\n",
    "        lists: the (per-epoch) costs on the evaluation data, the\n",
    "        accuracies on the evaluation data, the costs on the training\n",
    "        data, and the accuracies on the training data.  All values are\n",
    "        evaluated at the end of each training epoch.  So, for example,\n",
    "        if we train for 30 epochs, then the first element of the tuple\n",
    "        will be a 30-element list containing the cost on the\n",
    "        evaluation data at the end of each epoch. Note that the lists\n",
    "        are empty if the corresponding flag is not set.\n",
    "        \"\"\"\n",
    "        if evaluation_data: n_data = len(evaluation_data)\n",
    "        n = len(training_data)\n",
    "        evaluation_cost, evaluation_accuracy = [], []\n",
    "        training_cost, training_accuracy = [], []\n",
    "        for j in xrange(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [\n",
    "                training_data[k:k+mini_batch_size]\n",
    "                for k in xrange(0, n, mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(\n",
    "                    mini_batch, eta, lmbda, len(training_data))\n",
    "            print \"Epoch %s training complete\" % j\n",
    "            if monitor_training_cost:\n",
    "                cost = self.total_cost(training_data, lmbda)\n",
    "                training_cost.append(cost)\n",
    "                print \"Cost on training data: {}\".format(cost)\n",
    "            if monitor_training_accuracy:\n",
    "                accuracy = self.accuracy(training_data, convert=True)\n",
    "                training_accuracy.append(accuracy)\n",
    "                print \"Accuracy on training data: {} / {}\".format(\n",
    "                    accuracy, n)\n",
    "            if monitor_evaluation_cost:\n",
    "                cost = self.total_cost(evaluation_data, lmbda, convert=True)\n",
    "                evaluation_cost.append(cost)\n",
    "                print \"Cost on evaluation data: {}\".format(cost)\n",
    "            if monitor_evaluation_accuracy:\n",
    "                accuracy = self.accuracy(evaluation_data)\n",
    "                evaluation_accuracy.append(accuracy)\n",
    "                print \"Accuracy on evaluation data: {} / {}\".format(\n",
    "                    self.accuracy(evaluation_data), n_data)\n",
    "            print\n",
    "        return evaluation_cost, evaluation_accuracy, \\\n",
    "            training_cost, training_accuracy\n",
    "\n",
    "    def update_mini_batch(self, mini_batch, eta, lmbda, n):\n",
    "        \"\"\"Update the network's weights and biases by applying gradient\n",
    "        descent using backpropagation to a single mini batch.  The\n",
    "        ``mini_batch`` is a list of tuples ``(x, y)``, ``eta`` is the\n",
    "        learning rate, ``lmbda`` is the regularization parameter, and\n",
    "        ``n`` is the total size of the training data set.\n",
    "        \"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        self.weights = [(1-eta*(lmbda/n))*w-(eta/len(mini_batch))*nw\n",
    "                        for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(eta/len(mini_batch))*nb\n",
    "                       for b, nb in zip(self.biases, nabla_b)]\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "        \"\"\"Return a tuple ``(nabla_b, nabla_w)`` representing the\n",
    "        gradient for the cost function C_x.  ``nabla_b`` and\n",
    "        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar\n",
    "        to ``self.biases`` and ``self.weights``.\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x] # list to store all the activations, layer by layer\n",
    "        zs = [] # list to store all the z vectors, layer by layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        # backward pass\n",
    "        delta = (self.cost).delta(zs[-1], activations[-1], y)\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        # Note that the variable l in the loop below is used a little\n",
    "        # differently to the notation in Chapter 2 of the book.  Here,\n",
    "        # l = 1 means the last layer of neurons, l = 2 is the\n",
    "        # second-last layer, and so on.  It's a renumbering of the\n",
    "        # scheme in the book, used here to take advantage of the fact\n",
    "        # that Python can use negative indices in lists.\n",
    "        for l in xrange(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "\n",
    "    def accuracy(self, data, convert=False):\n",
    "        \"\"\"Return the number of inputs in ``data`` for which the neural\n",
    "        network outputs the correct result. The neural network's\n",
    "        output is assumed to be the index of whichever neuron in the\n",
    "        final layer has the highest activation.\n",
    "        The flag ``convert`` should be set to False if the data set is\n",
    "        validation or test data (the usual case), and to True if the\n",
    "        data set is the training data. The need for this flag arises\n",
    "        due to differences in the way the results ``y`` are\n",
    "        represented in the different data sets.  In particular, it\n",
    "        flags whether we need to convert between the different\n",
    "        representations.  It may seem strange to use different\n",
    "        representations for the different data sets.  Why not use the\n",
    "        same representation for all three data sets?  It's done for\n",
    "        efficiency reasons -- the program usually evaluates the cost\n",
    "        on the training data and the accuracy on other data sets.\n",
    "        These are different types of computations, and using different\n",
    "        representations speeds things up.  More details on the\n",
    "        representations can be found in\n",
    "        mnist_loader.load_data_wrapper.\n",
    "        \"\"\"\n",
    "        if convert:\n",
    "            results = [(np.argmax(self.feedforward(x)), np.argmax(y))\n",
    "                       for (x, y) in data]\n",
    "        else:\n",
    "            results = [(np.argmax(self.feedforward(x)), y)\n",
    "                        for (x, y) in data]\n",
    "        return sum(int(x == y) for (x, y) in results)\n",
    "\n",
    "    def total_cost(self, data, lmbda, convert=False):\n",
    "        \"\"\"Return the total cost for the data set ``data``.  The flag\n",
    "        ``convert`` should be set to False if the data set is the\n",
    "        training data (the usual case), and to True if the data set is\n",
    "        the validation or test data.  See comments on the similar (but\n",
    "        reversed) convention for the ``accuracy`` method, above.\n",
    "        \"\"\"\n",
    "        cost = 0.0\n",
    "        for x, y in data:\n",
    "            a = self.feedforward(x)\n",
    "            if convert: y = vectorized_result(y)\n",
    "            cost += self.cost.fn(a, y)/len(data)\n",
    "        cost += 0.5*(lmbda/len(data))*sum(\n",
    "            np.linalg.norm(w)**2 for w in self.weights)\n",
    "        return cost\n",
    "\n",
    "    def save(self, filename):\n",
    "        \"\"\"Save the neural network to the file ``filename``.\"\"\"\n",
    "        data = {\"sizes\": self.sizes,\n",
    "                \"weights\": [w.tolist() for w in self.weights],\n",
    "                \"biases\": [b.tolist() for b in self.biases],\n",
    "                \"cost\": str(self.cost.__name__)}\n",
    "        f = open(filename, \"w\")\n",
    "        json.dump(data, f)\n",
    "        f.close()\n",
    "\n",
    "#### Loading a Network\n",
    "def load(filename):\n",
    "    \"\"\"Load a neural network from the file ``filename``.  Returns an\n",
    "    instance of Network.\n",
    "    \"\"\"\n",
    "    f = open(filename, \"r\")\n",
    "    data = json.load(f)\n",
    "    f.close()\n",
    "    cost = getattr(sys.modules[__name__], data[\"cost\"])\n",
    "    net = Network(data[\"sizes\"], cost=cost)\n",
    "    net.weights = [np.array(w) for w in data[\"weights\"]]\n",
    "    net.biases = [np.array(b) for b in data[\"biases\"]]\n",
    "    return net\n",
    "\n",
    "#### Miscellaneous functions\n",
    "def vectorized_result(j):\n",
    "    \"\"\"Return a 10-dimensional unit vector with a 1.0 in the j'th position\n",
    "    and zeroes elsewhere.  This is used to convert a digit (0...9)\n",
    "    into a corresponding desired output from the neural network.\n",
    "    \"\"\"\n",
    "    e = np.zeros((10, 1))\n",
    "    e[j] = 1.0\n",
    "    return e\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"The sigmoid function.\"\"\"\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    return sigmoid(z)*(1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
